<!DOCTYPE HTML>
<html lang="en">
<head>
<meta charset="utf-8">
<title>RESEARCH | Kondo Lab, School of Psychology, Chukyo University</title>
<meta name="format-detection" content="telephone=no">
<meta http-equiv="X-UA-Compatible" content="IE=11">
<script src="js/jquery-1.11.2.min.js"></script>
<script src="js/script.js"></script>
<link href="css/style.css" rel="stylesheet" media="all">
<link href="css/slidebars.css" rel="stylesheet" media="all">
<script type="text/javascript">
if ((navigator.userAgent.indexOf('iPhone') > 0) || navigator.userAgent.indexOf('iPod') > 0 || navigator.userAgent.indexOf('Android') > 0) {
        //iPhone、iPod、Androidの設定
        document.write('<meta name="viewport" content="width=device-width,maximum-scale=1">');
    }else{
        //それ以外（PC、iPadなど）の設定
        document.write('<meta name="viewport" content="width=960,maximum-scale=1">');
    }
</script>
</head>
<body>
<div id="sb-site">
  <div id="header_frame">
    <header id="header" class="en">
      <h1><a href="index.html"><img src="image/etoptitle.png" alt="Kondo Lab, School of Psychology, Chukyo University"></a></h1>
      <div id="hnav_home"><a href="index.html"><img src="image/ehome.png" alt="HOME"></a></div>
      <div id="hnav_lang"><a href="research-j.html"><img src="image/elang.png" alt="JAPANESE"></a></div>
      <nav class="en">
        <ul id="menu">
          <li class="current"><a href="research.html">RESEARCH</a></li>
          <li><a href="publications.html">PUBLICATIONS</a></li>
          <li><a href="members.html">MEMBERS</a></li>
          <li><a href="access.html">ACCESS</a></li>
        </ul>
      </nav>
    </header>
  </div>
  <div id="container">
    <main>
      <section>
        <h2 class="mt0">Auditory and Visual Scene Analysis</h2>
        <figure class="research"><img src="image/icon_Ryoan-ji.jpg" alt=""></figure>
        <p class="justify">Our  brain is well equipped to transform a mixture of sensory inputs into  distinguishable meaningful object representations. We perceive the world as  stable, although auditory and visual inputs are often ambiguous due to spatial  and temporal occluders. This photo illustrates many of the cues and factors  that are involved in visual scene analysis: texture, contours, color, patterns  of light and shade, continuity, and occlusion. The visual scene leads us astray  into several interpretations, such as &quot;islands in an ocean&quot; or  &quot;a tiger carrying her cubs across the river&quot;. This raises important  questions regarding where and how this &ldquo;scene analysis&rdquo; is performed in the  brain. Recent advances from both auditory and visual research suggest that the  brain is not simply processing the incoming scene properties but that top-down  processes such as attention, expectations and prior knowledge facilitate scene  perception. Thus, scene analysis is linked not only with the formation and  selection of perceptual objects and stimulus properties, but also with  selective attention, perceptual binding, and awareness. (Photo by Chie Miki)</p>
        <h3 class="mt0">Reference</h3>
        <p class="justify"><u>Kondo,  H. M.</u>, van Loon, A., Kawahara, J.  I., &amp; Moore, B. C. J. (2017). Auditory and visual scene analysis: an overview. <strong>Philosophical Transactions of the Royal  Society of London. Series B: Biological Sciences</strong>, 372, 20160099. </p>
      </section>
      <section>
        <h2>Integration and Segregation of Auditory Streams</h2>
        <figure class="research"><img src="image/icon_stream.gif" alt=""></figure>
        <p class="justify">Various  sounds reach the ears together in time. However, one can follow a conversation  that is acoustically intermingled with competing conversations, loud music,  glasses tinkling, and so on. Humans and animals possess the sophisticated ability  to integrate complex acoustic inputs into one organized percept and switch  their attention among several auditory streams within inputs. The sequential  integration and segregation of frequency components for perceptual  organization, which called auditory streaming, is essential for auditory scene  analysis. Prolonged listening to an unchanging triplet-tone sequence produces a  series of illusory switches between a single coherent stream and two distinct  streams. The predominant percept depends on the frequency difference between  high and low tones. Our fMRI results demonstrated that the activity of the  auditory thalamus occurred earlier during switching from non-predominant to  predominant percepts, whereas that of the auditory cortex occurred earlier  during switching from predominant to non-predominant percepts. This suggest  that feed-forward and feedback processes in the thalamocortical loop are  involved in the formation of percepts in auditory streaming.</p>
        <h3>References</h3>
        <p class="justify"><u>Kondo,  H. M.</u>, &amp; Kashino, M. (2009).  Involvement of the thalamocortical loop in the spontaneous switching of  percepts in auditory streaming. <strong>Journal  of Neuroscience</strong>, 29, 12695-12701.</p>
        <p class="justify"><u>Kondo,  H. M.</u>, &amp; Kashino, M. (2007).  Neural mechanisms of auditory awareness underlying perceptual changes. <strong>NeuroImage</strong>, 36, 123-130.</p>
      </section>
      <section>
        <h2>Effects of Self-Motion on Auditory Scene Analysis</h2>
        <figure class="research"><img src="image/icon_telehead.png" alt=""></figure>
        <p class="justify">Many studies have been devoted to understanding the acoustic cues and, more recently, the neural mechanisms underlying auditory scene analysis. However, in all of these studies, a very odd cocktail party is considered: it is a cocktail party where the listener is unable to move his/her head. Therefore, very little is known about the interaction between sensory processes of auditory scene analysis and motor processes. Audio-motor processes related to head movements, which are known to help sound localization, should nevertheless be an important component of auditory scene analysis - both initially, as a listener actively explores a novel scene, and also in the ongoing maintenance of perceptual organization, as attention-grabbing sounds are likely to induce rapid head turns. Unfortunately, disentangling acoustic and motor cues is difficult because they are fully correlated for real movements. A virtual reality setup, the &quot;TeleHead&quot; system, was used to overcome this difficulty. We clarified effects of the following potential components on auditory streaming: changes in acoustic cues at the ears, changes in apparent source location, and changes in motor processes. The results demonstrated that the contribution of motor cues is lower than that of the two formers. (Photo by NTT CS Labs)</p>
        <h3>References</h3>
        <p class="justify"><u>Kondo,  H. M.</u>, Toshima, I., Pressnizer, D.,  &amp; Kashino, M. (2014). Probing the time course of head-motion cues  integration during auditory scene analysis. <strong>Frontiers in Neuroscience</strong>, 8, 170. (Invited  Article)</p>
        <p class="justify"><u>Kondo,  H. M.</u>, Pressnizer, D., Toshima, I.,  &amp; Kashino, M. (2012). The effects of self-motion on auditory scene  analysis. <strong>Proceedings of the National  Academy of Sciences of the United States of America</strong>, 109, 6775-6780. (Press Interest: <a href=" http://www.sciencemag.org/news/2012/04/robots-and-cocktails" target="_blank" class="blank">ScienceNOW</a>, <a href="http://www.wired.com/wiredscience/2012/04/robotic-head-hearing/" target="_blank" class="blank">WIRED</a>, <a href="http://www.popsci.com/technology/article/2012-04/video-creepy-humanoid-robot-mimics-human-head-movements-science" target="_blank" class="blank">Popular Science</a>, <a href="http://www.thenakedscientists.com/HTML/content/news-archive/news/2531/" target="_blank" class="blank">The Naked Scientists</a>, <a href="http://www.cosmosmagazine.com/node/5527" target="_blank" class="blank">COSMOS magazine</a>, <a href="http://www.huffingtonpost.com/2012/04/09/cocktail-party-effect_n_1413234.html" target="_blank" class="blank">Huffington Post</a>)</p>
      </section>
      <section>
        <h2>Separability and Commonality of Auditory and Visual Bistable Perception</h2>
        <figure class="research"><img src="image/icon_brainstem.gif" alt=""></figure>
        <p class="justify">Previous  studies on scene analysis have been conducted independently in the auditory and  visual domains. So it is difficult to obtain a bird&rsquo;s-eye view of  scene-analysis research. The present study used different forms of bistable  perception: auditory streaming, verbal transformations, visual plaids, and  reversible figures. We performed factor analyses on the number of perceptual  switches in the tasks. A three-factor model provided a better fit to the data  than the other possible models. These factors, namely, the &ldquo;auditory&rdquo;, &ldquo;shape&rdquo;  and &ldquo;motion&rdquo; factors, were separable but correlated with each other. We  compared the number of perceptual switches among genotype groups to identify  the effects of neurotransmitter functions on the factors. We focused on  polymorphisms of catechol-O-methyltransferase (COMT) Val158Met and serotonin 2A  receptor (HTR2A) -1438G/A genes, which are involved in the modulation of  dopamine and serotonin, respectively. The number of perceptual switches in  auditory streaming and verbal transformations differed among COMT genotype  groups, whereas that in reversible figures differed among HTR2A genotype  groups. The results indicate that the &ldquo;auditory&rdquo; and &ldquo;shape&rdquo; factors reflect  the functions of the dopamine and serotonin systems, respectively. Our findings  suggest that the formation and selection of percepts involve neural processes  in cortical and subcortical areas.</p>
        <h3>References</h3>
        <p class="justify"><u>Kondo,  H. M.</u>, Farkas, D., Denham, S. L.,  Asai, T., &amp; Winkler, I. (2017). Auditory multistability: idiosyncratic  perceptual switching patterns and neurotransmitter concentrations in the brain. <strong>Philosophical Transactions of the Royal  Society of London. Series B: Biological Sciences</strong>, 372, 20160110. <font color="red">Invited Article</font></p>
        <p class="justify"><u>Kondo,  H. M.</u>, Kitagawa, N., Kitamura, M.  S., Koizumi, A., Nomura, M., &amp; Kashino, M. (2012). The separability and  commonality of auditory and visual bistable perception. <strong>Cerebral Cortex</strong>, 22, 1915-1922.</p>
      </section>
    </main>
  </div>
  <a href="#header" id="page-top"><span class="arrow"></span></a>
  <div id="footer_frame">
    <footer><span class="style4">Kondo Lab, </span><span class="style3">School of Psychology, Chukyo University</span><br>
      Copyright &copy; Kondo Lab, School of Psychology, Chukyo University. All rights reserved.</footer>
  </div>
</div>
</body>
</html>